# 1. 基本概念

线性回归是一种基础的统计学习方法，用于建模因变量(目标变量)与一个或多个自变量(特征变量)之间的线性关系。它是最简单且应用最广泛的回归方法之一。

## 1.1 模型定义

对于简单线性回归，其数学表达式为：

$$y = wx + b$$
其中：
- $y$ 是因变量（预测值）
- $x$ 是自变量（特征值）
- $w$ 是权重（斜率）
- $b$ 是偏置项（截距）

对于多元线性回归，模型扩展为：

$$\hat{y} = w_1x_1 + w_2x_2 + ... + w_nx_n + b$$
用矩阵形式表示为：

$$\hat{}{y} = \mathbf{w^T}\mathbf{x} + \mathbf{b}$$
# 2. 模型假设

线性回归模型基于以下几个关键假设（也称为高斯-马尔可夫假设），这些假设的满足程度直接影响模型的有效性：

## 2.1 线性性假设（Linearity）

线性性假设要求自变量和因变量之间存在线性关系，可以表示为：

$$y_i = \mathbf{w}^\top\mathbf{x}_i + b + \epsilon_i$$

其中：

- $\epsilon_i$ 是误差项
- 线性关系指的是参数的线性性，而不是变量的线性性
- 变量本身可以进行非线性变换，如 $x^2$、$\log(x)$ 等

**违反此假设的后果**：

- 模型预测会产生系统性偏差
- 参数估计不准确
- 可能错过重要的非线性模式

**检验方法**：

- 残差图分析
- 散点图观察
- Ramsey RESET测试

## 2.2 独立性假设（Independence）

样本之间相互独立，即每个观测都是独立的随机事件，数学表示为：

$$\text{Cov}(\epsilon_i, \epsilon_j) = 0, \quad \forall i \neq j$$

协方差公式 $\text{Cov}(\epsilon_i, \epsilon_j) = 0, \quad \forall i \neq j$ .这个公式描述了误差项之间的独立性：
- $\epsilon_i$ 和 $\epsilon_j$ 是两个不同样本点的误差项
- $\text{Cov}$ 表示协方差，衡量两个随机变量的相关程度
- $\forall i \neq j$ 表示"对于任意不同的i和j"
- 等于0意味着任意两个不同样本点的误差之间没有线性相关性

举个例子：

- 如果我们在预测房价，这个假设意味着预测第一个房子的误差不会影响预测第二个房子的误差
- 违反这个假设的情况：时间序列数据中，前一天的预测误差可能会影响到后一天的预测误差

这意味着：

- 数据点之间没有序列相关性
- 一个样本的误差不应影响其他样本
- 不存在群体效应或时间依赖

**违反此假设的后果**：

- 标准误差估计偏小
- t统计量和F统计量不可靠
- 可能导致错误的显著性检验结果

**检验方法**：

- Durbin-Watson检验
- 自相关图
- 残差序列图分析

## 2.3 同方差性假设（Homoscedasticity）

误差项具有固定的方差，即：

$$\text{Var}(\epsilon_i) = \sigma^2, \quad \forall i$$

方差公式 $\text{Var}(\epsilon_i) = \sigma^2, \quad \forall i$ 这个公式描述了误差项的同方差性：
- $\text{Var}(\epsilon_i)$ 表示第i个误差项的方差
- $\sigma^2$ 是一个常数，表示固定的方差值
- $\forall i$ 表示"对于任意的i"
- 等式表明所有误差项都具有相同的方差
举个例子：
- 在预测房价时，这意味着无论是预测便宜的房子还是昂贵的房子，预测误差的波动程度都应该差不多
- 违反这个假设的情况：预测高价房的误差波动可能比预测低价房的误差波动更大
这两个假设结合起来，实际上是在说：
1. 不同样本之间的误差是相互独立的（第一个公式）
2. 所有样本的误差波动程度都是一样的（第二个公式）

当这两个假设都满足时，我们称误差项是"独立同分布"的（IID, Independent and Identically Distributed）。这是线性回归模型的重要基础。
需要注意的是，在实际应用中，这些假设很难完全满足，但我们需要检验它们的偏离程度，并在必要时采取适当的处理方法（比如使用加权最小二乘法或其他稳健回归方法）。

这要求：

- 误差的离散程度在所有自变量取值范围内保持一致
- 预测的不确定性不随自变量变化
- 残差图应呈现随机分散的带状结构

**违反此假设的后果**：

- OLS估计器不再是最有效的
- 标准误差估计有偏
- 置信区间和预测区间不准确

**检验方法**：

- White检验
- Breusch-Pagan检验
- 残差与预测值散点图
	
## 2.4 正态性假设（Normality）

误差项服从正态分布：

$\epsilon_i \sim \mathcal{N}(0, \sigma^2)$

这意味着：

- 误差的均值为0
- 误差的分布是对称的
- 大部分误差集中在均值附近

**违反此假设的后果**：

- 参数估计的置信区间可能不准确
- 假设检验的p值可能不可靠
- 在大样本情况下影响相对较小（中心极限定理）

**检验方法**：

- Q-Q图分析
- Shapiro-Wilk检验
- Kolmogorov-Smirnov检验

## 2.5 假设的重要性和处理方法

当这些假设被违反时，我们有以下几种处理方案：

1. **变量变换**：
    - 对因变量进行Box-Cox变换
    - 对自变量进行对数、平方根等变换
2. **使用稳健回归方法**：
    - 加权最小二乘法（WLS）
    - 稳健回归
    - 分位数回归
3. **模型调整**：
    - 增加非线性项
    - 考虑交互项
    - 使用更复杂的模型结构

# 3. 损失函数

## 1. 损失函数的基本概念

损失函数是衡量模型预测值与真实值之间差异的度量标准。它具有以下特点：

- 非负性：损失函数的值通常≥0
- 最优性：完美预测时损失为0
- 单调性：预测越准确，损失越小

### 1.1 解析解(Analytic Solution)
#### 1. 解析解概念

解析解是指通过数学公式直接计算得到的精确解，而不需要通过迭代优化来近似求解。线性回归是少数几个能够得到解析解的机器学习模型之一。

#### 2. 问题转化

### 2.1 将偏置并入权重向量

首先，我们通过特征扩充的方式将偏置项 $b$ 并入权重向量 $\mathbf{w}$ 中：

$$\tilde{\mathbf{x}}^{(i)} = \begin{bmatrix} \mathbf{x}^{(i)} \ 1 \end{bmatrix}, \quad \tilde{\mathbf{w}} = \begin{bmatrix} \mathbf{w} \ b \end{bmatrix}$$

扩充后的设计矩阵： $$\tilde{\mathbf{X}} = [\mathbf{X} \quad \mathbf{1}]$$

### 2.2 优化目标简化

优化问题转化为： $$\min_{\tilde{\mathbf{w}}} |\mathbf{y} - \tilde{\mathbf{X}}\tilde{\mathbf{w}}|_2^2$$

#### 3. 解析解推导

##### 3.1 求导过程

对平方损失求导： $$\begin{aligned} \frac{\partial}{\partial \tilde{\mathbf{w}}} |\mathbf{y} - \tilde{\mathbf{X}}\tilde{\mathbf{w}}|_2^2 &= \frac{\partial}{\partial \tilde{\mathbf{w}}} (\mathbf{y} - \tilde{\mathbf{X}}\tilde{\mathbf{w}})^\top(\mathbf{y} - \tilde{\mathbf{X}}\tilde{\mathbf{w}}) \ &= -2\tilde{\mathbf{X}}^\top(\mathbf{y} - \tilde{\mathbf{X}}\tilde{\mathbf{w}}) \ &= -2\tilde{\mathbf{X}}^\top\mathbf{y} + 2\tilde{\mathbf{X}}^\top\tilde{\mathbf{X}}\tilde{\mathbf{w}} \end{aligned}$$
**过程：**
1. 首先展开平方项： $$|\mathbf{y} - \tilde{\mathbf{X}}\tilde{\mathbf{w}}|_2^2 = (\mathbf{y} - \tilde{\mathbf{X}}\tilde{\mathbf{w}})^\top(\mathbf{y} - \tilde{\mathbf{X}}\tilde{\mathbf{w}})$$
2. 进一步展开： $$= \mathbf{y}^\top\mathbf{y} - \mathbf{y}^\top\tilde{\mathbf{X}}\tilde{\mathbf{w}} - \tilde{\mathbf{w}}^\top\tilde{\mathbf{X}}^\top\mathbf{y} + \tilde{\mathbf{w}}^\top\tilde{\mathbf{X}}^\top\tilde{\mathbf{X}}\tilde{\mathbf{w}}$$
3. 注意这里：$\mathbf{y}^\top\tilde{\mathbf{X}}\tilde{\mathbf{w}}$ 是标量，等于它的转置 $\tilde{\mathbf{w}}^\top\tilde{\mathbf{X}}^\top\mathbf{y}$
4. 所以可以写成： $$= \mathbf{y}^\top\mathbf{y} - 2\tilde{\mathbf{w}}^\top\tilde{\mathbf{X}}^\top\mathbf{y} + \tilde{\mathbf{w}}^\top\tilde{\mathbf{X}}^\top\tilde{\mathbf{X}}\tilde{\mathbf{w}}$$
5. 对 $\tilde{\mathbf{w}}$ 求导：
    - $\frac{\partial}{\partial \tilde{\mathbf{w}}}(\mathbf{y}^\top\mathbf{y}) = 0$ （常数项求导为0）
    - $\frac{\partial}{\partial \tilde{\mathbf{w}}}(-2\tilde{\mathbf{w}}^\top\tilde{\mathbf{X}}^\top\mathbf{y}) = -2\tilde{\mathbf{X}}^\top\mathbf{y}$
    - $\frac{\partial}{\partial \tilde{\mathbf{w}}}(\tilde{\mathbf{w}}^\top\tilde{\mathbf{X}}^\top\tilde{\mathbf{X}}\tilde{\mathbf{w}}) = 2\tilde{\mathbf{X}}^\top\tilde{\mathbf{X}}\tilde{\mathbf{w}}$
6. 最终得到： $$\frac{\partial}{\partial \tilde{\mathbf{w}}}|\mathbf{y} - \tilde{\mathbf{X}}\tilde{\mathbf{w}}|_2^2 = -2\tilde{\mathbf{X}}^\top\mathbf{y} + 2\tilde{\mathbf{X}}^\top\tilde{\mathbf{X}}\tilde{\mathbf{w}}$$

所以负号的来源是从展开二次项后，交叉项的求导得到的。这个负号是对的，因为我们在展开 $(\mathbf{y} - \tilde{\mathbf{X}}\tilde{\mathbf{w}})^\top(\mathbf{y} - \tilde{\mathbf{X}}\tilde{\mathbf{w}})$ 时，负号就出现在了交叉项中。

这个结果也可以通过维度分析来验证其正确性：最终的导数维度应该与 $\tilde{\mathbf{w}}$ 相同，这个结果确实满足这一点。
##### 3.2 最优解

令导数为零： $$-2\tilde{\mathbf{X}}^\top\mathbf{y} + 2\tilde{\mathbf{X}}^\top\tilde{\mathbf{X}}\tilde{\mathbf{w}} = 0$$

解得： $$\tilde{\mathbf{w}}^* = (\tilde{\mathbf{X}}^\top\tilde{\mathbf{X}})^{-1}\tilde{\mathbf{X}}^\top\mathbf{y}$$

#### 4. 解的存在性和唯一性

##### 4.1 存在条件

解析解存在的充分条件是矩阵 $\tilde{\mathbf{X}}^\top\tilde{\mathbf{X}}$ 可逆，这要求：

1. 样本数量 $n$ 必须大于等于特征数量 $d+1$
2. 设计矩阵 $\tilde{\mathbf{X}}$ 的列向量线性独立（满秩）

##### 4.2 唯一性

当上述条件满足时：

- 解是唯一的
- 这个解对应损失函数的全局最小值
- 不需要考虑初始值和学习率

#### 5. 实际应用考虑

##### 5.1 优点

1. 计算精确，一步到位
2. 不需要设置超参数
3. 保证找到全局最优解

##### 5.2 局限性

1. 计算复杂度高：$O(d^3)$，其中 $d$ 是特征维度
2. 对大规模数据不友好
3. 要求设计矩阵满秩
4. 不适用于在线学习

##### 5.3 数值稳定性

在实际实现时，通常使用以下方法提高数值稳定性：

1. 特征标准化
2. 使用QR分解或SVD分解
3. 添加正则化项（Ridge回归）

#### 6. 与其他优化方法的比较

##### 6.1 与梯度下降的对比

解析解：

- 一次性得到精确解
- 计算复杂度高
- 内存需求大

梯度下降：

- 迭代求解近似解
- 计算复杂度低
- 内存需求小
- 需要设置学习率等超参数

##### 6.2 使用场景

- 小规模数据集：优先使用解析解
- 大规模数据集：使用梯度下降或随机梯度下降
- 在线学习：只能使用梯度下降类方法

## 2. 均方误差损失（Mean Squared Error, MSE）

### 2.1 单样本损失

对于单个样本$i$，均方误差损失定义为：

$$l^{(i)}(\mathbf{w}, b) = \frac{1}{2}(\hat{y}^{(i)} - y^{(i)})^2$$

其中：

- $\hat{y}^{(i)} = \mathbf{w}^\top\mathbf{x}^{(i)} + b$ 是模型预测值
- $y^{(i)}$ 是真实值
- 系数 $\frac{1}{2}$ 是为了在求导时消除平方项的系数2

### 2.2 总体损失

对于包含$n$个样本的数据集，平均损失为：

$$L(\mathbf{w}, b) = \frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) = \frac{1}{n}\sum_{i=1}^n \frac{1}{2}(\mathbf{w}^\top\mathbf{x}^{(i)} + b - y^{(i)})^2$$
其中：
- $\frac{1}{n}$ 是平均因子，用于计算所有样本损失的平均值
- $\sum_{i=1}^n$ 表示对所有n个样本求和
- $\frac{1}{2}$ 是常数系数，用于在求导时消除平方项的2
- $\mathbf{w}^\top\mathbf{x}^{(i)}$ 是权重向量与第i个输入特征向量的内积
- $b$ 是偏置项
- $y^{(i)}$ 是第i个样本的真实值
- $(\mathbf{w}^\top\mathbf{x}^{(i)} + b - y^{(i)})$ 是预测值与真实值的差异
请注意，由于其二次形式，估计 $\hat{y}^{(i)}$ 和目标 $y^{(i)}$ 之间的巨大差异会导致对损失的贡献更大（这种二次性可能是一把双刃剑；虽然它鼓励模型以避免大错误（它也可能导致对异常数据过度敏感）。为了衡量 n 示例的整个数据集上的模型质量，我们只需对训练集上的损失进行平均（或等效地求和）即可, 这也是为什么需要一个平均因子


用矩阵形式可以更简洁地表示为：

$$L(\mathbf{w}, b) = \frac{1}{2n}|\mathbf{X}\mathbf{w} + b\mathbf{1} - \mathbf{y}|_2^2$$
其中：

- $\frac{1}{2n}$ 是系数，包含了平均因子和$\frac{1}{2}$
- $\mathbf{X}$ 是输入矩阵，维度为 $n \times d$（n个样本，每个样本d个特征）
- $\mathbf{w}$ 是权重向量，维度为 $d \times 1$
- $b$ 是标量偏置项
- $\mathbf{1}$ 是n维全1向量，维度为 $n \times 1$
- $\mathbf{y}$ 是真实值向量，维度为 $n \times 1$
- $|\cdot|_2^2$ 表示L2范数的平方（即欧氏距离的平方）

---

在训练模型时，我们寻找最优参数 $w^*$， $b^*$ 来最小化总损失函数，这可以表示为：

$$(w^*, b^*) = \underset{w, b}{\arg\min} \ L(w,b) = \underset{w, b}{\arg\min} \ \frac{1}{n}\sum_{i=1}^n \frac{1}{2}(w^\top x^{(i)} + b - y^{(i)})^2$$

让我来解释这个优化目标：

1. $\underset{w, b}{\arg\min}$ 表示：
    - 寻找使得损失函数取得最小值时的参数 $w$ 和 $b$
    - "arg min" 意思是"使函数最小的参数值"
2. 求解方法有两种： a) 解析解（直接求解）：
    
    - 将 $b$ 并入 $w$ 中，扩展特征矩阵
    - 通过令梯度为零求解： $$\begin{bmatrix} w^* \ b^* \end{bmatrix} = (\tilde{\mathbf{X}}^\top\tilde{\mathbf{X}})^{-1}\tilde{\mathbf{X}}^\top\mathbf{y}$$
    
    b) 迭代优化（梯度下降）：
    - 计算关于 $w$ 和 $b$ 的梯度
    - 沿梯度反方向更新参数： $$w := w - \alpha\frac{\partial L}{\partial w}$$ $$b := b - \alpha\frac{\partial L}{\partial b}$$ 其中 $\alpha$ 是学习率
3. 这个优化问题的特点：
    - 是一个凸优化问题
    - 有唯一的全局最优解
    - 可以通过线性代数或数值方法求解

## 3. MSE损失的特点

### 3.1 优点

1. **可导性**：处处二阶可导，便于优化
2. **凸函数**：具有唯一的全局最优解
3. **计算简单**：求导计算相对简单
4. **物理意义明确**：表示预测偏差的平方和

### 3.2 缺点

1. **对异常值敏感**：由于平方项的存在，离群点会产生较大影响
2. **尺度依赖**：损失值依赖于目标变量的尺度
3. **惩罚不对称**：高估和低估的惩罚是一样的

## 4. 其他常用损失函数

### 4.1 平均绝对误差（Mean Absolute Error, MAE）

$$L_{MAE}(\mathbf{w}, b) = \frac{1}{n}\sum_{i=1}^n |y^{(i)} - \hat{y}^{(i)}|$$

特点：

- 对异常值更不敏感
- 在中位数处最小化
- 不是处处可导

### 4.2 Huber损失

结合了MSE和MAE的优点：

$$ L_{Huber}(\mathbf{w}, b) = \frac{1}{n}\sum_{i=1}^n \begin{cases} \frac{1}{2}(y^{(i)} - \hat{y}^{(i)})^2 & \text{if } |y^{(i)} - \hat{y}^{(i)}| \leq \delta \ \delta|y^{(i)} - \hat{y}^{(i)}| - \frac{1}{2}\delta^2 & \text{otherwise} \end{cases} $$

特点：

- 对小误差使用平方损失
- 对大误差使用线性损失
- δ是一个超参数，控制转换点

## 5. 损失函数的选择

选择损失函数时需要考虑：

1. **数据分布**：是否存在异常值
2. **任务需求**：对过估计和低估的敏感度
3. **优化难度**：是否需要处处可导
4. **计算效率**：计算和优化的复杂度

## 6. 损失函数与优化

### 6.1 梯度计算

对于MSE损失，其梯度为：

$$\frac{\partial L}{\partial \mathbf{w}} = \frac{1}{n}\mathbf{X}^\top(\mathbf{X}\mathbf{w} + b\mathbf{1} - \mathbf{y})$$

$$\frac{\partial L}{\partial b} = \frac{1}{n}\mathbf{1}^\top(\mathbf{X}\mathbf{w} + b\mathbf{1} - \mathbf{y})$$

### 6.2 最优化方法

1. **解析解**： $$\begin{bmatrix} \mathbf{w}^* \ b^* \end{bmatrix} = (\mathbf{\tilde{X}}^\top\mathbf{\tilde{X}})^{-1}\mathbf{\tilde{X}}^\top\mathbf{y}$$
2. **梯度下降**： $$\mathbf{w} := \mathbf{w} - \alpha\frac{\partial L}{\partial \mathbf{w}}$$ $$b := b - \alpha\frac{\partial L}{\partial b}$$



# 4. The Normal Distribution and Squared Loss

## 1. 理论基础

1. **从概率角度理解线性回归**
- 线性回归不仅仅是一个优化问题，也可以从概率的角度来理解
- 当我们说 $y = w^\top x + b + \epsilon$ 时，其中的 $\epsilon$ 是噪声项
- 假设这个噪声服从正态分布 $\epsilon \sim N(0,\sigma^2)$，这是一个很自然的假设，因为：
    - 实际数据中的误差通常来自多个小的随机因素的叠加
    - 根据中心极限定理，多个独立随机变量的和趋向于正态分布

2. **为什么要用平方损失**
- 平方损失函数的选择不是随意的，它直接来自于正态分布的假设
- 当我们假设噪声服从正态分布时，通过最大似然估计（MLE）可以推导出：
    - 似然函数：$P(y|x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{(y-w^\top x-b)^2}{2\sigma^2})$
    - 取对数并最小化负对数似然，就得到了平方损失函数
    - 也就是说，最小化平方损失等价于在正态噪声假设下的最大似然估计

3. **实际意义**
- 这种理论基础帮助我们理解线性回归的适用场景：
    - 当数据的噪声确实近似正态分布时，线性回归是最优的
    - 如果噪声分布严重偏离正态，可能需要考虑其他模型或损失函数
- 也帮助我们理解模型的局限性和假设条件

4. 噪声（noise）或者误差项（error term）$\epsilon$ 在现实中来自多个来源：
- **测量误差**：
    - 仪器精度限制
    - 测量过程中的随机波动
    - 人为记录错误
- **未观测变量的影响**：
    - 我们的模型没有包含所有相关的解释变量
    - 某些重要因素可能难以测量或无法获取
    - 例如：预测房价时可能遗漏了一些细节特征
- **模型简化导致的误差**：
    - 线性假设本身的局限性
    - 忽略了变量间的复杂交互关系
    - 将非线性关系简化为线性关系
- **随机性事件**：
    - 环境因素的随机波动
    - 不可预测的外部干扰
    - 时间变化带来的随机影响
- **数据收集过程中的误差**：
    - 采样误差
    - 数据记录过程中的随机错误
    - 数据处理和转换过程中的误差

这就是为什么假设噪声服从正态分布是合理的：

- 根据中心极限定理，多个独立随机因素的叠加趋向于正态分布
- 这些误差源通常是众多小的、独立的随机因素共同作用的结果
- 每个单独的误差源可能很小，但它们的累积效应显著

举个例子：预测房价时的噪声可能来自：
- 测量房屋面积时的误差
- 未能记录的细节（如装修质量）
- 市场情绪的随机波动
- 临时性的环境因素（如附近施工）

### 1.1 线性回归的概率解释

在线性回归中，我们通常写模型为：

$$y = w^\top x + b + \epsilon$$

其中：
- $y$ 是因变量（目标值）
- $w^\top x + b$ 是确定性部分（预测值）
- $\epsilon$ 是随机噪声项

### 1.2 噪声的正态分布假设

假设噪声项服从正态分布：

$$\epsilon \sim \mathcal{N}(0, \sigma^2)$$

这个假设的合理性来自：
- 实际数据中的误差通常来自多个微小随机因素的叠加
- 根据中心极限定理，大量独立随机变量的和近似服从正态分布

## 2. 从概率到损失函数

### 2.1 条件概率表示

给定输入 $x$，输出 $y$ 的条件概率为：

$$P(y|x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(y-w^\top x-b)^2}{2\sigma^2}\right)$$

### 2.2 最大似然估计

对于整个数据集，似然函数为：

$$P(y|X) = \prod_{i=1}^n P(y^{(i)}|x^{(i)})$$

取对数：

$$\log P(y|X) = \sum_{i=1}^n \log P(y^{(i)}|x^{(i)})$$

$$= -\sum_{i=1}^n \left[\frac{1}{2}\log(2\pi\sigma^2) + \frac{(y^{(i)}-w^\top x^{(i)}-b)^2}{2\sigma^2}\right]$$

### 2.3 从最大似然到平方损失

最小化负对数似然等价于：

$$\min_{w,b} \sum_{i=1}^n (y^{(i)}-w^\top x^{(i)}-b)^2$$

这就是我们熟悉的平方损失函数！

## 3. 深入理解

### 3.1 为什么使用平方损失
1. **数学推导**：从正态分布假设出发，通过最大似然估计自然得到
2. **统计意义**：在正态噪声假设下，最小二乘估计就是最大似然估计
3. **实践效果**：对大多数实际问题，平方损失都能得到不错的结果

### 3.2 模型假设
1. **噪声独立性**：各样本的噪声相互独立
2. **同方差性**：噪声的方差恒定
3. **正态性**：噪声服从正态分布

### 3.3 应用意义

理解正态分布与线性回归的关系有助于：
1. 判断模型适用性
2. 解释模型预测结果
3. 评估模型假设的合理性
4. 选择合适的优化方法

## 4. 实际应用考虑

### 4.1 检验假设
1. QQ图检验正态性
2. 残差图检验同方差性
3. 自相关检验独立性

### 4.2 处理违反假设的情况
1. **非正态**：考虑使用其他损失函数
2. **异方差**：使用加权最小二乘法
3. **非独立**：考虑时间序列模型

## 5. 总结

正态分布与线性回归的关系不仅是历史巧合，更反映了：
1. 数学理论的优美统一性
2. 概率视角对机器学习的重要性
3. 模型假设对实际应用的指导意义

理解这种关系有助于：
- 更深入地理解模型原理
- 更准确地判断模型适用性
- 更合理地改进模型表现


# 6. 泛化
## 1. 训练误差与泛化误差

### 1.1 基本定义与IID假设

首先，我们假设训练数据和测试数据是从相同分布独立采样的（IID假设）：
- 训练数据 $(X, y)$ 和测试数据 $(X', y')$ 来自相同分布 $P(X,Y)$
- 每个样本之间相互独立

在此假设下，我们定义：

1. **训练误差**（经验误差）：
$R_{emp}[X,y,f] = \frac{1}{n}\sum_{i=1}^n l(x^{(i)}, y^{(i)}, f(x^{(i)}))$

2. **泛化误差**（期望误差）：
$R[p,f] = \mathbb{E}_{(x,y)\sim P}[l(x,y,f(x))] = \iint l(x,y,f(x))p(x,y)dxdy$

关键区别：
- 训练误差是在有限训练集上的统计量
- 泛化误差是在底层分布上的期望值
- 我们永远无法精确计算泛化误差（因为不知道真实分布），只能通过测试集估计

其中：
- $h$ 是假设函数（学习到的模型）
- $L$ 是损失函数
- $P_{data}$ 是真实的数据分布

### 1.2 泛化间隙（Generalization Gap）

泛化间隙定义为泛化误差与训练误差的差：
$$\text{Gap}(h) = E_{gen}(h) - E_{train}(h)$$

## 2. 模型复杂度

### 2.1 参数数量
对于多项式回归：
$$h_d(x) = \sum_{i=0}^d w_ix^i$$
其中 $d$ 是多项式的阶数，决定了模型的复杂度。

### 2.2 复杂度度量
- VC维（Vapnik-Chervonenkis维度）
- Rademacher复杂度
- 模型参数的L1/L2范数：
  $$\|w\|_1 = \sum_{i=1}^d |w_i|$$
  $$\|w\|_2 = \sqrt{\sum_{i=1}^d w_i^2}$$

## 3. 过拟合与欠拟合

### 3.1 数学描述

欠拟合（高偏差）：
$$E_{train}(h) \gg E_{optimal}$$

过拟合（高方差）：
$$E_{gen}(h) \gg E_{train}(h)$$

其中 $E_{optimal}$ 是可达到的最优误差。

### 3.2 偏差-方差分解
均方误差可以分解为：
$$\mathbb{E}[(h(x)-y)^2] = \text{Bias}^2(h(x)) + \text{Var}(h(x)) + \sigma^2$$

其中：
- Bias: 预测的期望与真实值的差
- Var: 预测值的方差
- $\sigma^2$: 不可约误差

## 4. 多项式曲线拟合

### 4.1 多项式曲线拟合

给定单特征训练数据 $x$ 和对应的实值标签 $y$，d阶多项式模型：

$\hat{y} = \sum_{i=0}^d x^iw_i$

其中：
- $d$ 是多项式的阶数，决定了模型复杂度
- $x^0 = 1$ 对应偏置项
- 高阶多项式比低阶多项式更复杂，因为：
  1. 参数数量更多
  2. 模型函数的选择范围更广
  3. 可以完美拟合最多 d+1 个不同的点

复杂度与拟合关系：
- 当 d 很小时，可能欠拟合
- 当 d 接近或超过样本数时，容易过拟合
- 最优的 d 值需要通过验证集来确定

### 4.2 正则化
添加L2正则化项：
$$L_{reg}(w) = \frac{1}{n}\sum_{i=1}^n(y^{(i)} - f_d(x^{(i)}))^2 + \lambda\|w\|_2^2$$

## 5. 数据集大小的影响

### 5.1 学习曲线
训练误差和验证误差与训练集大小m的关系：
$$E_{train}(m) \text{ 和 } E_{val}(m)$$

### 5.2 渐近行为
当 $m \to \infty$ 时：
$$\lim_{m \to \infty} E_{train}(m) \approx \lim_{m \to \infty} E_{val}(m)$$

## 6. 模型选择

### 6.1 验证集方法
将数据分为：
- 训练集 $D_{train}$
- 验证集 $D_{val}$
- 测试集 $D_{test}$

### 6.2 选择准则
选择在验证集上表现最好的模型：
$$h^* = \arg\min_{h \in \mathcal{H}} E_{val}(h)$$

## 7. 交叉验证

### 7.1 K折交叉验证
K折CV误差：
$$E_{cv} = \frac{1}{K}\sum_{k=1}^K E_{val}^{(k)}$$

其中 $E_{val}^{(k)}$ 是第k折的验证误差。

### 7.2 留一交叉验证（LOOCV）
特殊情况K=n时：
$$E_{loocv} = \frac{1}{n}\sum_{i=1}^n L(h^{(-i)}(x^{(i)}), y^{(i)})$$

其中 $h^{(-i)}$ 是在去除第i个样本后训练的模型。

## 8. 模型评估指标

### 8.1 回归问题常用指标

1. 均方误差（MSE）：
$$\text{MSE} = \frac{1}{n}\sum_{i=1}^n(y^{(i)} - \hat{y}^{(i)})^2$$

2. 平均绝对误差（MAE）：
$$\text{MAE} = \frac{1}{n}\sum_{i=1}^n|y^{(i)} - \hat{y}^{(i)}|$$

3. 决定系数（R²）：
$$R^2 = 1 - \frac{\sum_{i=1}^n(y^{(i)} - \hat{y}^{(i)})^2}{\sum_{i=1}^n(y^{(i)} - \bar{y})^2}$$

### 8.2 评估流程

1. 采用嵌套交叉验证：
   - 外层CV：评估泛化性能
   - 内层CV：执行模型选择

2. 统计显著性检验
   - 配对t检验
   - Wilcoxon符号秩检验


# 7. 权重衰减（Weight Decay）

## 1. 引入背景与动机

### 1.1 过拟合问题
在机器学习中，过拟合是一个普遍存在的问题。虽然我们可以通过收集更多的训练数据来缓解过拟合，但这往往代价高昂或不可行。因此，我们需要在给定数据集的条件下，探索其他可用的工具来解决过拟合问题。

### 1.2 复杂度控制
在多项式回归等问题中，我们可以通过调整多项式的次数来限制模型容量。然而，对于高维输入，这种方法可能过于粗糙。例如，对于k个变量的d次多项式，其项数为：

$$ \binom{k-1+d}{k-1} $$

这个数量随着d的增加会迅速爆炸，使得我们需要一个更细粒度的工具来调整函数复杂度。

## 2. 权重衰减的数学基础

### 2.1 范数与复杂度度量
权重衰减的核心思想是通过限制参数可以取的值来控制模型复杂度。其基本假设是，在所有函数中，f=0（对所有输入都赋值为0的函数）在某种意义上是"最简单"的，我们可以通过测量函数参数与零的距离来衡量函数的复杂度。

### 2.2 范数选择
对于线性函数 $f(x)=w^Tx$，我们可以通过其权重向量的某个范数来度量其复杂度，最常用的是$ℓ_2$范数：

$$ \|w\|_2 = \sqrt{\sum_{i=1}^n w_i^2} $$

### 2.3 损失函数的修改
将范数作为惩罚项添加到原始损失函数中，得到新的优化目标：

原始损失函数：
$$ L(w,b) = \frac{1}{n}\sum_{i=1}^n\frac{1}{2}(w^Tx^{(i)}+b-y^{(i)})^2 $$

添加权重衰减后的损失函数：
$$ L_{new}(w,b) = \frac{1}{n}\sum_{i=1}^n\frac{1}{2}(w^Tx^{(i)}+b-y^{(i)})^2 + \frac{\lambda}{2}\|w\|_2^2 $$

其中：
- $\lambda$ 是正则化系数（非负超参数）
- $\frac{\lambda}{2}\|w\|_2^2$ 是正则化项
- 系数$\frac{1}{2}$是为了在求导时消除平方项的2

## 3. 权重衰减的特性分析

### 3.1 不同范数的效果比较
1. $ℓ_2$范数（权重衰减）：
   - 对权重向量的大分量施加较大惩罚
   - 倾向于在所有特征上均匀分布权重
   - 提高对单个变量测量误差的鲁棒性

2. $ℓ_1$范数（Lasso回归）：
   - 倾向于产生稀疏解
   - 可以实现特征选择
   - 适用于希望获得稀疏模型的场景

### 3.2 优化过程分析
使用小批量随机梯度下降进行优化时，权重的更新规则为：

$$ w \leftarrow (1-\eta\lambda)w - \frac{\eta}{|B|}\sum_{i\in B}x^{(i)}(w^Tx^{(i)}+b-y^{(i)}) $$

这个更新规则可以分解为两部分：
1. 权重衰减项：$(1-\eta\lambda)w$
2. 梯度更新项：$-\frac{\eta}{|B|}\sum_{i\in B}x^{(i)}(w^Tx^{(i)}+b-y^{(i)})$

注意到权重衰减项会使权重按比例收缩，这也是"权重衰减"名称的由来。

## 4. 实践中的注意事项

### 4.1 偏置项处理
通常不对偏置项b进行正则化，原因包括：
1. 偏置项不会导致过拟合
2. 我们希望模型能够无偏地学习数据的中心趋势
3. 偏置项的维度与数据维度无关

### 4.2 超参数选择
选择正则化系数$\lambda$时需要考虑：
1. 数据集规模：数据量越大，通常需要较小的$\lambda$
2. 特征数量：特征数量越多，可能需要较大的$\lambda$
3. 噪声水平：噪声越大，通常需要较大的$\lambda$
4. 通过交叉验证选择最优值

### 4.3 实现技巧
1. 数据预处理：
   - 特征标准化很重要，确保权重衰减对所有特征的惩罚是公平的
   - 建议使用StandardScaler进行预处理

2. 参数初始化：
   - 合理的参数初始化可以加快收敛
   - 避免初始权重过大或过小

3. 学习率调整：
   - 权重衰减可能需要调整学习率
   - 考虑使用学习率衰减策略

## 5. 高维线性回归实例

考虑如下的高维线性回归问题：

$$ y = 0.05 + \sum_{i=1}^d 0.01x_i + \epsilon \quad \text{where } \epsilon \sim N(0, 0.01^2) $$

这个示例中：
1. 输入维度d = 200
2. 训练样本仅有20个
3. 验证样本100个
4. 噪声服从均值为0，标准差为0.01的高斯分布

这种情况下：
- 不使用权重衰减时，模型容易过拟合
- 使用适当的权重衰减可以显著改善模型的泛化性能

## 6. 扩展应用

### 6.1 深度学习中的应用
在深度神经网络中，权重衰减通常：
1. 应用于所有层的权重
2. 不同层可以使用不同的衰减系数
3. 与其他正则化技术组合使用

### 6.2 非线性扩展
对于非线性函数，可以考虑：
1. 再生核希尔伯特空间(RKHS)方法
2. 分层正则化策略
3. 自适应正则化方法

## 7. 实现示例

```python
class LinearRegressionWithWeightDecay:
    def __init__(self, lambda_param=1.0):
        self.lambda_param = lambda_param
        
    def loss(self, X, y, w, b):
        predictions = X.dot(w) + b
        mse = np.mean((predictions - y) ** 2)
        l2_penalty = self.lambda_param * np.sum(w ** 2)
        return mse + l2_penalty
        
    def fit(self, X, y):
        # 实现训练逻辑
        pass
```