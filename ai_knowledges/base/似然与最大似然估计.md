## 1. 似然（Likelihood）的概念

### 1.1 基本定义
似然是一个统计学概念，表示在给定某一参数值的情况下，观测到当前数据的概率。

- 概率：已知参数，预测数据的可能性
- 似然：已知数据，参数取某值的合理性

### 1.2 数学表达
对于参数 $\theta$ 和观测数据 $x$：
- 概率：$P(x|\theta)$ 看作关于 $x$ 的函数
- 似然：$L(\theta|x) = P(x|\theta)$ 看作关于 $\theta$ 的函数

### 1.3 理解区别
假设抛硬币的例子：
- 概率：已知硬币正面概率为0.6，求抛3次得到2次正面的概率
- 似然：已知抛3次得到2次正面，求硬币正面概率为0.6的合理性

## 2. 最大似然估计（Maximum Likelihood Estimation, MLE）

### 2.1 基本思想
选择一个参数值，使得观测到当前数据的概率最大。

数学表达：
$$\theta_{MLE} = \arg\max_{\theta} L(\theta|x)$$
1. **符号含义**：
   - $\theta_{MLE}$ 是最大似然估计的参数值
   - $L(\theta|x)$ 是似然函数，表示在观测到数据 $x$ 时，参数取值为 $\theta$ 的似然度
   - $\arg\max_{\theta}$ 表示"使得函数取得最大值时的参数值"

2. **举个例子**：抛硬币
   - $\theta$ 是硬币正面朝上的概率
   - $x$ 是观测数据，比如"10次抛掷中有7次正面"
   - $L(\theta|x)$ 是在参数为 $\theta$ 时观察到这个结果的概率
   - 我们要找到让这个概率最大的 $\theta$ 值

3. **具体计算**：
   对于抛硬币例子：
   ```
   L(θ|x) = C(10,7) * θ^7 * (1-θ)^3
   ```
   - 对 L(θ|x) 求导
   - 令导数为0
   - 解出 θ = 0.7

4. **直观理解**：
   - 就像"反向思考"：已知结果，反推最可能的参数
   - 相当于问："什么样的参数值最可能产生我们观察到的数据？"

这就是为什么叫"最大似然估计"：我们在找一个参数值，使得观察到当前数据的"似然性"最大。
### 2.2 对数似然
通常使用对数似然，因为：
1. 便于计算（乘积变为求和）
2. 数值稳定性更好
3. 不改变最优解（对数函数单调）

$$\log L(\theta|x) = \sum_{i=1}^n \log P(x_i|\theta)$$

1. **为什么使用对数似然**

原始的似然函数是所有样本概率的乘积：
$$L(\theta|x) = \prod_{i=1}^n P(x_i|\theta)$$

取对数后变成：
$$\log L(\theta|x) = \sum_{i=1}^n \log P(x_i|\theta)$$

使用对数的主要原因：

a) **计算优势**：
- 将乘积转换为求和，计算更简单
- 避免数值下溢（很多小数相乘会得到接近零的数）
- 便于计算梯度和导数

b) **数值稳定性**：
- 原始似然函数可能非常小（比如 0.1 的 100 次方）
- 对数能将很小的数映射到合理范围内
- 防止计算机精度问题

c) **不改变最优解**：
- 对数函数是单调递增的
- $\arg\max_{\theta} L(\theta|x) = \arg\max_{\theta} \log L(\theta|x)$
- 最大值的位置保持不变

2. **实际例子**

假设有投掷硬币的数据：
- 原始似然：$L(\theta) = \theta^7(1-\theta)^3$ （10次投掷，7次正面）
- 对数似然：$\log L(\theta) = 7\log(\theta) + 3\log(1-\theta)$

对数形式：
- 更容易求导
- 数值更稳定
- 计算更简单

3. **在线性回归中的应用**

当假设噪声服从正态分布时：
- 原始似然：
$$L(w,b) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(y^{(i)}-w^\top x^{(i)}-b)^2}{2\sigma^2}\right)$$

- 对数似然：
$$\log L(w,b) = -\sum_{i=1}^n \left[\frac{1}{2}\log(2\pi\sigma^2) + \frac{(y^{(i)}-w^\top x^{(i)}-b)^2}{2\sigma^2}\right]$$

这样就更容易看出它与平方损失函数的关系。

所以，对数似然不改变最大似然估计的结果，但使得计算过程更加简单和稳定。这是一个非常实用的数学技巧，在机器学习和统计学中广泛使用。
### 2.3 步骤
1. 写出似然函数
2. 取对数（如果需要）
3. 求导并令导数为零
4. 解方程得到估计值

## 3. 线性回归中的最大似然

### 3.1 假设
假设噪声服从正态分布：
$$y = w^\top x + b + \epsilon, \quad \epsilon \sim \mathcal{N}(0,\sigma^2)$$

### 3.2 似然函数
对单个样本：
$$P(y^{(i)}|x^{(i)}; w,b) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(y^{(i)}-w^\top x^{(i)}-b)^2}{2\sigma^2}\right)$$

对所有样本：
$$L(w,b) = \prod_{i=1}^n P(y^{(i)}|x^{(i)}; w,b)$$

### 3.3 对数似然
$$\log L(w,b) = -\sum_{i=1}^n \left[\frac{1}{2}\log(2\pi\sigma^2) + \frac{(y^{(i)}-w^\top x^{(i)}-b)^2}{2\sigma^2}\right]$$

### 3.4 最大化对数似然
等价于最小化：
$$\sum_{i=1}^n (y^{(i)}-w^\top x^{(i)}-b)^2$$

这就是我们熟悉的平方损失函数！

## 4. 最大似然估计的特点

### 4.1 优点
1. 理论基础扎实
2. 具有良好的统计性质
3. 渐进无偏性
4. 渐进有效性

### 4.2 局限性
1. 需要知道数据分布
2. 可能对异常值敏感
3. 小样本情况下可能不稳定
4. 可能存在多个局部最优解

## 5. 实际应用

### 5.1 应用领域
1. 参数估计
2. 模型选择
3. 假设检验
4. 机器学习模型训练

### 5.2 注意事项
1. 样本独立性假设
2. 分布假设的合理性
3. 数值计算的稳定性
4. 优化算法的选择

## 6. 与其他方法的比较

### 6.1 贝叶斯估计
- MLE：参数是固定的未知常数
- 贝叶斯：参数是随机变量

### 6.2 矩估计
- MLE：考虑完整的分布信息
- 矩估计：只考虑数据的矩