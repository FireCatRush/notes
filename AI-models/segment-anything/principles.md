
# 本文贡献
segment anything 模型由facebook meta AI团队开发，通过三个互相关联的组件构建了一个基础模型，这三个基础组件分别是：
- a prompt-able segmentation task（可提示的分割任务）
- a segmentation model (SAM) that powers data annotation and enables zero-shot transfer to a range of tasks via prompt engineering （一个驱动数据注释并通过提示工程实现零样本转移到各种任务的分割模型（SAM））
- a data engine for collecting SA-1B, our dataset of over 1 billion masks （一个用于收集SA-1B（超过10亿个掩码数据集）的数据引擎。）
# 摘要
+ 分割数据集：数据集包含1100万张图像（11M ），超过10亿个mask掩码
+ prompt-able model: 可zero shot 迁移至新的distribution和task -> 零样本性能优于prior fully supervisied results
# Introduction
解决以下三个问题：
1. 什么**任务**将实现零样本推理？
2. 对应的**模型**架构是什么？
3. 什么**数据**可以支持这个任务和模型？

## 1. 任务：
promptable task -> is to return a valid segmentation mask given any segmentation prompt 
## 2. 模型：
一个强大的图像编码器计算图像嵌入，一个提示编码器嵌入提示，然后这两种信息源结合在一个轻量级掩码解码器中，预测分割掩码。通过将SAM分为图像编码器和快速提示编码器/掩码解码器，同一图像嵌入可以与不同的提示一起重新使用（并摊销其成本）。给定图像嵌入，提示编码器和掩码解码器可以在网页浏览器中以约50ms从提示预测出掩码。我们专注于点、框和掩码提示，也介绍了自由形式文本提示的初步结果。
```
The promptable segmentation task and the goal
of real-world use impose constraints on the model architec-
ture. In particular, the model must support flexible prompts,
needs to compute masks in amortized real-time to allow in-
teractive use, and must be ambiguity-aware. Surprisingly,
we find that a simple design satisfies all three constraints:
a powerful image encoder computes an image embedding,
a prompt encoder embeds prompts, and then the two infor-
mation sources are combined in a lightweight mask decoder
that predicts segmentation masks. We refer to this model as
the Segment Anything Model, or SAM (see Fig. 1b). By
separating SAM into an image encoder and a fast prompt
encoder / mask decoder, the same image embedding can
be reused (and its cost amortized) with different prompts.
Given an image embedding, the prompt encoder and mask
decoder predict a mask from a prompt in ∼50ms in a web
browser. We focus on point, box, and mask prompts, and
also present initial results with free-form text prompts. To
make SAM ambiguity-aware, we design it to predict mul-
tiple masks for a single prompt allowing SAM to naturally
handle ambiguity, such as the shirt vs. person example
```
> 可提示的分割任务和实际使用中的目标对模型架构提出了约束。特别是，模型必须支持灵活的提示，需要以摊销后的实时计算掩码以允许交互使用，并且必须对歧义具有感知能力。令人惊讶的是，我们发现一个简单的设计可以满足所有这三项约束：一个强大的图像编码器计算图像嵌入，一个提示编码器嵌入提示，然后这两种信息源结合在一个轻量级掩码解码器中，预测分割掩码。我们将这种模型称为Segment Anything Model，或简称SAM（见图1b）。通过将SAM分为图像编码器和快速提示编码器/掩码解码器，同一图像嵌入可以与不同的提示一起重新使用（并摊销其成本）。给定图像嵌入，提示编码器和掩码解码器可以在网页浏览器中以约50ms从提示预测出掩码。我们专注于点、框和掩码提示，也介绍了自由形式文本提示的初步结果。为了使SAM具有歧义感知，我们设计它以便为单个提示预测多个掩码，使SAM自然处理歧义，例如衬衫与人的示例。


# Segment Anything Task

prompt可以是一组前景/背景点、一个粗糙的框或掩码、自由形式的文本（指示在图像中分割什么的任何信息），根据提示返回一个有效的分割掩码。“有效”意味着，即使提示模棱两可，可以引用多个对象（例如衬衫与人的例子），输出也至少是其中一个合理的掩码。类似于期望一个语言模型对一个模糊的提示输出一个一致的响应。

1. **可提示分割任务**：这段内容介绍了一种被称为“可提示分割任务”（promptable segmentation task）的预训练算法。在这个任务中，模型会针对每个训练样本模拟一系列的提示（如点、框、掩码），然后将模型预测的掩码与真实标签（ground truth）进行比较。
    
2. **方法的来源**：这种方法借鉴了交互式分割（interactive segmentation）的方法[109, 70]。在交互式分割中，模型的目标是通过逐步接收用户输入，最终预测出一个有效的掩码。
    
3. **任务目标的不同**：与交互式分割不同，这里的目标是无论提示是否明确，模型都能即时预测出一个有效的掩码。这意味着即使提示是模糊的（例如一个点可能对应多个对象），模型也必须能够输出一个合理的掩码。这种设定确保了预训练模型在处理涉及模糊提示的应用场景（如自动注释）时也能表现良好。
    
4. **挑战和需求**：要在这个任务上表现良好是具有挑战性的，因为这需要专门的建模和训练损失选择。文章在第3节中会进一步讨论这些专门的建模和训练策略。

根据文章内容，作者的主要工作组件是**可提示分割模型**（promptable segmentation model）。以下是其主要工作方式的详细解释：

1. **任务泛化能力**：可提示分割模型具备一种称为任务泛化（task generalization）的能力。这意味着该模型可以在推理（inference）时执行与训练任务不同的新任务。这与多任务分割系统（multi-task segmentation systems）不同。多任务分割系统中的单一模型执行一组固定的任务（如联合语义分割、实例分割和全景分割），这些任务在训练和测试阶段都是一致的。

2. **模块化设计**：可提示分割模型被设计为一个灵活的组件，可以在不同的系统中用于不同的任务。例如，为了执行实例分割任务，模型可以与现有的对象检测器结合使用。在实例分割任务中，首先使用对象检测器检测图像中的对象并生成边界框，然后将这些边界框作为提示输入到可提示分割模型中，模型会根据这些提示生成相应的实例分割掩码。

3. **具体工作方式**：
   - **训练阶段**：在训练阶段，可提示分割模型会模拟一系列提示（如点、框、掩码）并将模型的掩码预测与真实标签进行比较。该方法借鉴了交互式分割，但目标是无论提示是否明确，模型都能即时预测出一个有效的掩码。
   - **推理阶段**：在推理阶段，模型可以通过与其他组件（如对象检测器）的结合来执行新的任务。例如，在实例分割任务中，模型会根据对象检测器生成的边界框提示来生成分割掩码。

总结来说，作者的主要工作组件是**可提示分割模型**，它通过处理各种提示（如点、框、掩码）来生成分割掩码。与多任务分割系统不同，这种模型能够在推理时执行与训练任务不同的新任务，通过与其他组件（如对象检测器）的结合来实现复杂的分割任务。

# Segment Anything Model
![[Segment Anything Model overview.png]]

如上图所示，Segment Anything 有三个部分组成：an image encoder, a flexible prompt encoder, and a fast mask decoder.

模型基于Transform Vision, 并对实时性能做了权衡

**Image encoder.**  , 受可扩展性和强大的预训练方法的启发，我们使用了 MAE 预训练的 Vision Transformer (ViT)，该模型经过最低限度的调整，可以处理高分辨率输入。图像编码器每幅图像运行一次，可以在提示模型之前应用。

**Prompt encoder.**  我们考虑两组提示：稀疏（点、框、文本）和密集（掩码）。我们用位置编码表示点和框，并用 CLIP 提供的现成文本编码器对每种提示类型和自由格式文本的学习嵌入进行求和。密集提示（即掩码）使用卷积进行嵌入，并与图像嵌入逐元素求和。

**Mask decoder**. 掩码解码器有效地将图像嵌入、提示嵌入和输出标记映射到掩码。此设计受到"Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with Transformers" 和 “Bowen Cheng, Alex Schwing, and Alexander Kirillov. Perpixel classification is not all you need for semantic segmentation” 的启发，采用了 Transformer 解码器块 的修改，后跟动态掩码预测头。我们修改后的解码器块在两个方向（提示到图像嵌入和反之亦然）使用提示自注意和交叉注意来更新所有嵌入。运行两个块后，我们对图像嵌入进行上采样，MLP 将输出标记映射到动态线性分类器，然后计算每个图像位置的掩码前景概率

**Resolving ambiguity**. 如果给出一个模糊提示，模型将使用一个输出对多个有效掩码进行平均。
为了解决这个问题，我们修改了模型，以预测单个提示的多个输出掩码（见图 3）。我们发现 3 个掩码输出足以解决大多数常见情况（嵌套掩码通常最多为三层：整体、部分和子部分）。在训练期间，我们仅反向传播掩码上的最小损失。为了对掩码进行排序，模型会预测每个掩码的置信度分数（即估计的 IoU）。

效率。整体模型设计主要受效率驱动。给定预先计算的图像嵌入，提示编码器和掩码解码器在 CPU 上的网络浏览器中运行，大约需要 50 毫秒。这种运行时性能使我们的模型能够无缝、实时地进行交互式提示。

损失和训练。我们使用 [14] 中使用的焦点损失 [65] 和骰子损失 [73] 的线性组合来监督掩码预测。我们使用混合的几何提示来训练可提示的分割任务（有关文本提示，请参阅 §7.5）。按照 [92, 37]，我们通过对每个掩码随机抽样 11 轮提示来模拟交互式设置，从而使 SAM 能够无缝集成到我们的数据引擎中。





# Segment Anything Data Engine

由于互联网上的分割掩码并不丰富，作者建立了一个数据引擎来收集1.1亿掩码数据集SA-1B。数据引擎有三个阶段：（1）模型辅助的手动注释阶段（2）混合了自动预测掩码和模型辅助注释的半自动阶段，以及（3）模型在没有注释器输入的情况下生成掩码的全自动阶段。
辅助手动阶段在第一阶段，类似于经典的交互式分割，一组专业注释人员通过使用SAM提供的基于浏览器的交互式分割工具点击前景/背景对象点来标记mask。mask可以使用像素级“画笔”和“橡皮擦”工具进行细化。模型辅助注释直接在浏览器中实时运行（使用预先计算的图像嵌入），从而实现真正的交互式体验。作者没有对标记对象施加语义约束，注释器可以自由地标记“东西”和“事物”。作者建议注释器标记他们可以命名或描述的对象，但没有收集这些名称或描述。注释者被要求按照突出的顺序标记对象，并被鼓励在mask注释超过30秒后继续下一张图像。
在这个阶段开始时，SAM是使用公共分割数据集进行训练的。在充分的数据注释之后，仅使用新注释的掩码对SAM进行再训练。随着更多掩模的收集，图像编码器从ViT-B扩展到ViT-H，其他架构细节也在发展；作者总共对模型进行了6次训练。随着模型的改进，每个掩码的平均注释时间从34秒减少到14秒。14秒比COCO的掩码注释快6.5倍，仅比使用极值点的边界框标记慢2倍。随着SAM的改进，每张图像的平均掩模数量从20个增加到44个。总的来说，在这个阶段从120k张图像中收集了430万个mask。
半自动阶段 在这个阶段，作者的目标是增加mask的多样性，以提高模型分割任何东西的能力。为了将注释器集中在不太突出的对象上，首先自动检测到自信的掩码。然后，向注释器展示了预先填充了这些掩码的图像，并要求他们注释任何其他未注释的对象。为了检测有信心的掩码，使用通用的“对象”类别在所有第一阶段掩码上训练了一个边界框检测器。在此阶段，在180k张图像中额外收集了590万个掩模（总共1020万个掩膜）。与第一阶段一样，定期根据新收集的数据对模型进行再训练（5次）。每个掩码的平均注释时间回到了34秒（不包括自动掩码），因为这些对象更难标记。每张图像的平均mask数量从44个增加到72个（包括自动mask）。
全自动阶段 在最后阶段，注释是完全自动的。这是可行的，因为模型有两个主要的增强。首先，在这个阶段开始时，作者收集了足够的mask来大大改进模型，包括前一阶段的各种mask。其次，到了这个阶段，已经开发了模糊感知模型，它能够预测有效的掩码，即使在模糊的情况下也是如此。具体来说，用32×32的规则网格提示模型，并为每个点预测一组可能对应于有效对象的掩码。使用模糊感知模型，如果一个点位于部分或子部分上，模型将返回子部分、部分和整个对象。模型的IoU预测模块用于选择置信掩码；此外，只识别并选择了稳定的掩码（如果在0.5−δ和0.5+δ处对概率图进行阈值处理会导致类似的掩码，则认为掩码是稳定的）。最后，在选择了置信和稳定的掩码后，应用非最大抑制（NMS）来过滤重复。为了进一步提高较小mask的质量，还处理了多个重叠的放大图像裁剪。


# Segment Anything Dataset

### **SA-1B 数据集的掩码属性**

#### **1. 空间分布（Spatial Distribution）**

![[mask_spatial_distribution.png]]
- **对象中心分布**：SA-1B 的对象中心分布在图像中的覆盖范围较广，尤其在**图像角落**部分，相较于 **LVIS v1** 和 **ADE20K** 更均匀。
- **中心偏差**：相比之下，**COCO** 和 **Open Images V5** 数据集在对象分布上存在明显的**中心偏差**，即对象更集中在图像中心区域。

---
#### **2. 数据集规模（Dataset Size）**

- **图像数量**：SA-1B 包含 **11倍** 于第二大数据集（Open Images）的图像数量。
- **掩码数量**：SA-1B 包含 **400倍** 于 Open Images 的分割掩码数量。
- **每图掩码数**：
    - SA-1B 平均每张图像包含的掩码数量是 Open Images 的 **36倍**。
    - 与 ADE20K 相比，SA-1B 的平均每图掩码数仍是 **3.5倍**。

---
#### **3. 掩码尺寸（Mask Size）**

- **相对尺寸**：通过计算掩码面积与图像面积的比值（掩码面积的平方根除以图像面积的平方根），SA-1B 数据集显示：
    - 由于每张图像包含更多掩码，SA-1B 的掩码分布中**小型和中型掩码**的比例更高。

---
#### **4. 掩码形状复杂度（Mask Shape Complexity）**

- **复杂度定义**：使用**凹度（Concavity）**进行衡量，公式为： 1−掩码面积掩码凸包面积1 - \frac{\text{掩码面积}}{\text{掩码凸包面积}}1−掩码凸包面积掩码面积​
- **分析方法**：为了排除掩码尺寸分布对复杂度的影响，研究人员进行了**分层抽样**，控制了不同掩码尺寸的分布。
- **结果**：SA-1B 的掩码凹度分布与其他主要数据集（如 COCO、ADE20K）大致相似，表明掩码形状复杂度在不同数据集中保持一致性。

---
### **总结**

- **空间分布**：SA-1B 的对象中心分布更加均匀，特别是在图像角落部分，减少了中心偏差。
- **数据规模**：SA-1B 在图像数量、掩码总量和每图掩码数量方面均远超现有数据集。
- **掩码尺寸**：小型和中型掩码占比更高，符合高掩码密度的特性。
- **掩码复杂度**：尽管掩码数量庞大，但其形状复杂度与其他主流数据集保持一致。


# 7. Zero-Shot Transfer Experiments

#### **1. 实验背景与目标**

- **目标**：评估 Segment Anything 模型（SAM）在**零样本迁移任务**上的性能，即在**未见过的任务和数据集**上进行分割。
- **任务**：涵盖五个任务，其中四个任务与 SAM 训练时使用的**可提示分割任务**显著不同。
- **数据集**：包含新的图像分布，如**水下图像**或**第一人称视角图像**，这些图像在 SA-1B 数据集中并未出现过。
- **零样本迁移**：此处的“零样本迁移”与 CLIP 模型的定义相同，即在未见过的任务上进行推理。

---
#### **2. 核心任务：提示分割（Promptable Segmentation）**

- **目标**：从任何提示中生成有效的分割掩码。
- **挑战场景**：
    - 重点关注**单个前景点提示**的情况，因为这种提示往往比其他更具体的提示更具歧义性。
    ps: 单个前景点是指在图像分割任务中，用户或算法提供的一个**单一坐标点**，该点位于目标对象（前景）上，作为模型的**提示（Prompt）**，引导模型在图像中识别并分割出与该点相关联的前景对象。

---
#### **3. 实验任务**

SAM 被设计用于一个可提示的分割任务，但通过**提示工程（Prompt Engineering）**，模型被测试在以下四个不同任务上：

1. **边缘检测（Edge Detection）**
    
    - 识别图像中物体的边界线。
    - 要求模型具备对低级视觉特征的理解能力。
2. **分割所有对象（Segment Everything）**
    
    - 生成对象提议（Object Proposal Generation）。
    - 要求模型能够检测图像中的所有潜在对象。
3. **实例分割（Instance Segmentation）**
    
    - 对检测到的每个对象进行单独分割。
    - 要求模型能够对中级视觉特征进行建模。
4. **基于自由文本的分割（Segment from Free-Form Text）**
    
    - 使用自然语言描述对对象进行分割。
    - 例如，“分割图中的蓝色物体”或“分割所有人类”。
    - 这一任务作为一个**概念验证**实验，展示 SAM 在高级视觉理解任务上的潜力。

---
#### **4. 实现细节（Implementation Details）**

- **模型架构**：
    - SAM 使用**MAE（Masked Autoencoder）** 预训练的 **ViT-H（Vision Transformer - Huge）** 图像编码器。
- **训练数据**：
    - SAM 在 **SA-1B 数据集**上进行训练。
    - SA-1B 仅包含由数据引擎最后阶段自动生成的掩码。
- **更多细节**：
    - 其他模型和训练细节（如超参数设置）可参见论文附录部分（§A）。

---
### **5. 关键亮点**

- SAM 在**多个未见过的任务**上展示了良好的零样本迁移能力。
- **任务跨度大**：从低级的边缘检测到高级的基于文本的自由分割，覆盖了不同层次的视觉理解需求。
- **提示工程**：通过提示工程，SAM 能够在超出其初始训练任务范围的情况下，灵活地完成各种分割任务。

## **7.1 零样本单点有效掩码评估（Zero-Shot Single Point Valid Mask Evaluation）**

#### **1. 任务描述（Task Description）**

- **目标**：评估 SAM（Segment Anything Model）在仅通过**单个前景点（Single Foreground Point）**的提示下，分割出一个有效的前景对象掩码的能力。
    
- **挑战性**：
    
    - **多义性（Ambiguity）**：单个点可能对应多个对象，导致任务**不适定**（Ill-posed）。
    - **真值掩码的局限性**：大多数数据集的标注不会枚举所有可能的掩码，这可能导致自动化指标（如 mIoU）不可靠。
- **评估指标**：
    
    - **mIoU（Mean Intersection over Union）**：所有预测掩码与真值掩码的 IoU 均值。
    - **人工评分（Human Study）**：标注者对分割掩码进行评分，评分范围为 **1（无意义）到 10（像素级完美）**。
- **点的选择**：
    
    - 默认情况下，从**真值掩码的中心点**进行采样（使用掩码内部距离变换的最大值）。
    - SAM 默认选择其**置信度最高的掩码**进行评估。
- **对比基准（Baselines）**：
    
    - **RITM**：一种强大的交互式分割模型，被认为是当前该任务的最佳基线。
    - **其他模型**：SimpleClick 和 FocalClick。

---

#### **2. 数据集（Datasets）**

- **数据集套件**：使用了一个包含 **23个数据集** 的新套件，这些数据集具有**多样化的图像分布**（包括水下图像、第一人称图像等）。
- **评估范围**：
    - **mIoU 评估**：在所有 23 个数据集上进行。
    - **人工评分**：由于资源限制，仅在部分子集上进行。

---

#### **3. 实验结果（Results）**

##### **(1) 自动化评估（Automatic Evaluation - mIoU）**

- **整体表现**：SAM 在 **23个数据集中的16个**上表现优于 RITM，最高可领先 **47 IoU**。
- **Oracle 模型**：
    - 如果使用一个“Oracle”方法（从 SAM 生成的 3 个掩码中选择与真值最匹配的掩码，而非默认选择置信度最高的掩码），SAM 在**所有数据集上**均表现优于 RITM。
    - 这表明**歧义性**对自动化评估有显著影响。

##### **(2) 人工评分（Human Study）**

- **结果表现**：
    - SAM 的分割掩码在**人工评分**中显著高于 RITM。
    - SAM 的平均评分在 **7到9之间**，对应的定性描述为：
        - **“高分（7-9）：对象是可识别的，错误小且罕见（例如，遗漏了一个小的、被严重遮挡的独立部分）。”**
- **消融实验**：
    - 去除多掩码预测（仅输出单一掩码）的版本，得分较标准 SAM 低，但仍显著高于 RITM。
- **特定数据集**：在如 **DRAM 和 IBD** 数据集中，尽管 SAM 在 mIoU 上劣于 RITM，但在**人工评分**中仍然获得更高分。

##### **(3) 多点提示性能**

- 当提示点的数量从 **1增加到9** 时：
    - **性能差距缩小**：随着提示点的增加，任务变得更简单，各模型间的差距逐渐减小。
    - SAM 并未针对**极高 IoU 场景**进行优化。

##### **(4) 随机点采样 vs 中心点采样**

- 将**默认中心点采样**替换为**随机点采样**：
    - **SAM 的优势依然明显**：无论是中心点采样还是随机点采样，SAM 都能保持较高的性能水平。

---

#### **4. 关键发现（Key Findings）**

1. **强大的单点提示性能**：即使在单一前景点的条件下，SAM 也能够生成高质量的有效掩码。
2. **歧义性影响**：多义性是自动评估性能的一个限制因素，但 SAM 通过多掩码预测有效地缓解了这一问题。
3. **人工评分优越性**：在定性和定量上，SAM 的掩码质量均超过了基线模型（RITM、SimpleClick、FocalClick）。
4. **多点提示平滑性能**：随着提示点数量的增加，各模型之间的性能差距缩小。
5. **稳定的性能表现**：无论是**中心点采样**还是**随机点采样**，SAM 都能维持高性能输出。

---

#### **5. 总结**

- SAM 在**零样本单点分割任务**中表现出色，能够在未见过的图像和任务上生成高质量的掩码。
- 即使在单点提示的模糊和不确定性下，SAM 依然展示出对对象边界的准确捕捉能力。
- SAM 的设计（包括多掩码预测和强大的图像编码器）使其在面对多义性时表现更加稳健。
- **人类评分结果**进一步证实了 SAM 在实际分割任务中的有效性和实用性。


## **7.2 零样本边缘检测（Zero-Shot Edge Detection）**

#### **1. 方法（Approach）**

- **任务目标**：评估 SAM 在**边缘检测（Edge Detection）**任务上的性能。
- **数据集**：使用经典的低级视觉任务数据集 **BSDS500**。
- **提示策略**：
    - 使用一个 **16×16 的规则网格**对图像进行均匀采样，共 **256个前景点**。
    - 每个点生成 **3个预测掩码**，总计 **768个掩码**。
    - 使用**非极大值抑制（NMS）** 移除冗余掩码。
- **边缘图生成**：
    - 使用**Sobel滤波器**对未阈值化的掩码概率图进行处理，生成初始边缘图。
    - 进行标准的轻量级后处理，包括**边缘NMS**（非极大值抑制）。
- **更多细节**：见论文附录 §D.2。

---

#### **2. 实验结果（Results）**

##### **(1) 定性分析（Qualitative Analysis）**

- **边缘可视化**：代表性边缘图在**图10**中展示（更多示例见**图15**）。
- **观察结果**：
    - 尽管 SAM 并未专门为边缘检测任务进行训练，但它依然能够生成**合理的边缘图**。
    - SAM 检测到的边缘数量较多，甚至包括一些在 **BSDS500** 数据集中**未标注但合理**的边缘。
    - 这表明 SAM 更倾向于**全面地检测所有可能的边缘**，而非严格遵循数据集的偏好。

##### **(2) 定量分析（Quantitative Analysis）**

- **指标**：主要使用 **50%精度下的召回率（R50）** 进行比较。
- **结果总结**：
    - **召回率较高，精度较低**：SAM 检测到了更多的边缘，但部分边缘可能是冗余或非必要的。
    - **对比 SOTA 方法**：SAM 的表现自然落后于**当前最先进的边缘检测方法**，这些方法在训练时明确学习了 **BSDS500** 数据集的偏好（例如哪些边缘应该被抑制）。
    - **与经典方法对比**：SAM 的性能显著优于早期的**零样本边缘检测方法**，甚至在一定程度上接近经典的深度学习边缘检测方法，如 **HED（Holistically-Nested Edge Detection）**。

---

#### **3. 关键发现（Key Findings）**

1. **泛化能力强**：尽管 SAM 并未针对边缘检测任务进行特定训练，但依然能够生成高质量的边缘图。
2. **高召回率 vs 低精度**：SAM 倾向于检测更多边缘，可能包括一些不属于**BSDS500真值**的边缘。
3. **合理的冗余边缘**：即使检测到的边缘比真值更多，许多边缘在语义上是合理的，并未偏离物体的结构特征。
4. **与 SOTA 方法的差距**：SAM 未能达到专门为 BSDS500 训练的模型的性能水平，因为这些模型学会了**抑制数据集中不需要的边缘**。
5. **优于早期零样本方法**：SAM 显著超越了早期的零样本方法，证明了其强大的泛化性和稳健性。

---

#### **4. 总结（Summary）**

- SAM 展现出卓越的**零样本边缘检测**能力，尽管没有针对该任务进行训练，依然能够生成**合理且细致的边缘图**。
- 在召回率上表现突出，但在精度上略显不足，这反映出 SAM 的泛化能力强于任务特定性。
- 与早期的零样本方法相比，SAM 显著提升了性能，接近经典的深度学习边缘检测方法。
- SAM 的结果强调了**可提示分割模型（Promptable Segmentation Models）**在多样化视觉任务上的潜力，显示出在低级视觉任务（如边缘检测）上的广泛适用性。


## **7.3 零样本目标提议（Zero-Shot Object Proposals）**

#### **1. 方法（Approach）**

- **任务目标**：评估 SAM 在**目标提议生成（Object Proposal Generation）**任务上的性能。
    
- **任务背景**：
    
    - 目标提议是目标检测任务的重要中间步骤。
    - 在早期目标检测系统（例如 Selective Search、R-CNN 等）中，目标提议生成是必不可少的环节。
- **实现细节**：
    
    - 使用 SAM 的**自动掩码生成管道**的略微修改版本。
    - 将生成的掩码作为目标提议（Object Proposals）输出。
    - 更多细节可见附录 §D.3。
- **评估指标**：
    
    - 使用 **平均召回率（Average Recall, AR）** 作为评估指标。
- **数据集**：
    
    - 使用 **LVIS v1 数据集**，因为它包含大量类别，能够有效测试模型在多类别场景下的泛化性能。
- **对比基线（Baselines）**：
    
    - **ViTDet-H**：基于 **ViT-H** 和 **Cascade Mask R-CNN** 实现的强大目标检测器。
    - 该基线实际上是**DMP（Detector Masquerading as Proposal Generator）** 方法，这是一种经过优化以在 AR 指标上取得高分的策略。
    - 此基线为 SAM 提供了一个**极具挑战性**的比较对象。

---

#### **2. 实验结果（Results）**

##### **(1) 整体表现（Overall Performance）**

- **ViTDet-H**（DMP 方法）在整体指标上表现最佳，这并不意外，因为该模型专门在 LVIS 数据集上进行训练并针对性优化。
- **SAM 的表现**：
    - 在**中等大小和大型对象**上，SAM 的表现超过了 ViTDet-H。
    - 在**稀有类别和常见类别**上，SAM 同样超过了 ViTDet-H。
    - **小型对象**和**高频类别**上，ViTDet-H 表现更好，这是因为 ViTDet-H 在 LVIS 上进行了特定训练，能够学习到 LVIS 数据集的标注偏差。

##### **(2) 消融实验（Ablation Study）**

- **单掩码输出版本（Ambiguity-Unaware Version, Single Out）**：
    - 去除多掩码预测的版本在所有 AR 指标上都显著落后于标准 SAM。
    - 这表明 SAM 的多掩码预测策略在应对歧义性时具有显著优势。

---

#### **3. 关键发现（Key Findings）**

1. **出色的零样本目标提议性能**
    
    - SAM 在**中等大小和大型对象**以及**稀有和常见类别**上表现优越。
    - 即使没有在 LVIS 数据集上进行特定训练，SAM 依然在多个指标上接近甚至超越了 ViTDet-H。
2. **小型对象和频繁类别的局限性**
    
    - SAM 在小型对象和高频类别上略逊于 ViTDet-H，这主要是因为 ViTDet-H 在 LVIS 数据集上进行了优化。
    - 这表明 SAM 在处理高密度、小尺度对象时可能存在一定的局限性。
3. **多掩码策略的有效性**
    
    - 消融实验表明，SAM 的**多掩码预测策略**显著提升了模型的性能，特别是在应对模糊和歧义性场景时。
4. **泛化能力突出**
    
    - SAM 在未经过特定任务和数据集训练的情况下，依然展示出极强的泛化能力，特别是在多类别目标提议生成任务上。

---

#### **4. 总结（Summary）**

- **强大的泛化能力**：SAM 在**零样本目标提议**任务上表现出色，能够生成高质量的目标提议掩码。
- **与任务特定模型的对比**：虽然在小型对象和高频类别上略显劣势，但在**中等/大型对象**和**稀有/常见类别**上，SAM 的表现超过了专门在 LVIS 数据集上训练的 ViTDet-H。
- **消融实验的验证**：多掩码预测策略显著提高了 SAM 的性能。
- **关键意义**：SAM 展示了其作为**通用视觉分割基础模型**的潜力，能够在不同任务中实现稳健的零样本迁移性能。

## **7.4 零样本实例分割（Zero-Shot Instance Segmentation）**

---
#### **1. 方法（Approach）**

- **任务目标**：将 **SAM（Segment Anything Model）** 集成到一个**实例分割（Instance Segmentation）**系统中。
    
- **实现细节**：
    
    - 使用一个目标检测器（**ViTDet**）来检测图像中的目标对象。
    - 将目标检测器输出的**边界框（Bounding Boxes）** 作为提示传递给 **SAM**，以生成分割掩码。
    - 该方法展示了 **SAM** 在一个更大的视觉系统中的**模块化应用**。
- **评估数据集**：
    
    - **COCO 数据集**
    - **LVIS 数据集**
- **评估指标**：
    
    - **掩码 AP（Mask Average Precision）**

---

#### **2. 实验结果（Results）**

##### **(1) 定量分析（Quantitative Analysis）**

- **性能比较**：
    - 在 **COCO** 和 **LVIS** 数据集上，**SAM** 的掩码 AP 分数与 **ViTDet** 存在一定的差距。
    - **SAM** 的表现**接近** ViTDet，但在特定指标上略有落后。

##### **(2) 定性分析（Qualitative Analysis）**

- **掩码质量**：
    - 虽然在 AP 指标上落后，但 **SAM** 生成的分割掩码在**边界清晰度**上往往优于 **ViTDet**。
    - **SAM** 的掩码具有**更清晰、锐利的边界**，特别是在复杂对象的分割中。
- **可视化**：可在 **图16** 中看到更多示例。

##### **(3) 人工评分（Human Study）**

- **评分方法**：使用 **1到10** 的质量评分标准，要求标注者对 **SAM** 和 **ViTDet** 的掩码进行评分。
- **结果**：
    - 人工评分中，**SAM 一致性地优于 ViTDet**。
    - 即使在指标上略显劣势，**人类观察者更倾向于认为 SAM 生成的掩码质量更高**。

##### **(4) 原因分析（Hypothesis）**

- **COCO 数据集**：
    - **COCO** 的掩码标注质量相对较低，存在特定的标注偏差。
    - **ViTDet** 能够在训练中学习并利用这些偏差来优化性能，而 **SAM** 作为一个**零样本方法**，无法利用这些偏差。
- **LVIS 数据集**：
    - **LVIS** 的掩码标注质量较高，但存在特定的结构偏差（例如：掩码不包含孔洞，构建为简单多边形）。
    - **ViTDet** 能够学习这些特定偏差，而 **SAM** 无法学习这些数据集特有的特性。

---

#### **3. 关键发现（Key Findings）**

1. **边界质量更高**
    
    - **SAM** 在分割边界的锐利度和精确度上优于 **ViTDet**。
2. **泛化性能突出**
    
    - 尽管 **SAM** 未在 COCO 和 LVIS 上进行专门训练，但它仍然在许多情况下能够生成高质量的实例分割掩码。
3. **数据集偏差影响**
    
    - **ViTDet** 能够学习数据集特有的标注偏差，从而在标准指标上略胜一筹。
    - **SAM** 的泛化能力使其更能在多样化的图像分布中表现稳定。
4. **人类评分优势**
    
    - 尽管在 AP 上略逊于 ViTDet，但在人类评分中，**SAM 的掩码更受认可**，反映出其在实际任务中的可用性更强。

---

#### **4. 总结（Summary）**

- **SAM** 在实例分割任务中展现出**高质量的分割边界**，尽管在掩码 AP 指标上略逊于 **ViTDet**，但在人工评分中获得了更高的评价。
- **ViTDet** 在训练中学习了特定数据集的标注偏差，从而在特定指标上表现更优。
- **SAM** 的泛化性能确保了其在未见过的数据集和任务中依然能够保持稳健的性能。
- 这一实验展示了 **SAM** 作为一个模块化分割组件，可以被无缝集成到更大的视觉系统中，完成复杂的高层视觉任务。

---

## **7.5 零样本文本到掩码（Zero-Shot Text-to-Mask）**

#### **1. 方法（Approach）**

- **任务目标**：使用**自由形式文本提示**生成分割掩码。
- **创新训练策略**：
    - SAM 未直接使用文本进行训练，而是使用了 **CLIP 图像嵌入（Image Embeddings）** 进行训练。
    - **训练过程**：
        - 对每个手动收集的面积大于 **100²** 的掩码，提取其对应的 **CLIP 图像嵌入**。
        - 训练期间，使用图像嵌入作为 SAM 的第一个交互提示。
    - **推理过程**：
        - 在推理时，文本通过 **CLIP 文本编码器** 转换为文本嵌入。
        - 将文本嵌入作为提示传递给 **SAM** 进行分割。

---

#### **2. 实验结果（Results）**

- **定性分析**：
    - **简单提示**：例如 "a wheel"（一个轮子），SAM 能够正确生成掩码。
    - **复杂提示**：例如 "beaver tooth grille"（海狸齿格栅），SAM 同样能够成功分割。
    - **失败场景**：
        - 对于模糊或不明确的文本提示，SAM 可能失败。
        - 添加一个**额外的点提示**通常能够纠正错误。

---

#### **3. 关键发现（Key Findings）**

1. **文本与分割的连接**
    
    - SAM 能够理解文本提示并生成相应的分割掩码。
2. **点提示的补充**
    
    - 当仅依靠文本提示失败时，添加一个**单点提示**通常可以纠正预测。

---

#### **4. 总结（Summary）**

- SAM 成功展示了其在**文本到分割掩码**任务上的潜力。
- 通过 CLIP 的图像-文本对齐嵌入，SAM 能够在零样本的情况下进行文本驱动的分割任务。
- **文本提示 + 点提示** 的组合进一步提高了模型的鲁棒性和准确性。

这些结果表明，SAM 具备在未来应用中进行**跨模态任务**的潜力，为视觉任务与自然语言任务的结合开辟了新的可能性。



## **7.6 消融实验（Ablations）**

#### **1. 目标（Objective）**

- **验证不同训练配置对 SAM 性能的影响。**
- **探索数据源、数据量和模型架构（ViT 编码器）的影响。**
- **基于单中心点提示（Single Center Point Prompt Protocol）** 进行测试。

---
### **2. 消融实验设计（Experimental Design）**

#### **(1) 数据源（Data Source）**

- **训练数据阶段**：SAM 的训练数据来源于数据引擎的三个阶段：**手动标注、半自动标注、自动标注**。
- **数据分布**：
    - **自动生成掩码**在最终数据集中占主导地位，数量远超手动和半自动掩码。
    - 为了平衡数据分布，**对手动和半自动掩码进行10倍过采样（Oversampling）**。
- **对比设置**：
    - **所有数据（All Data）**：包括手动、半自动和自动掩码。
    - **仅自动掩码（Automatic Masks Only）**：仅使用自动生成的掩码进行训练。

**结果（Fig.13 左侧）：**

- 每个数据引擎阶段的引入都会提升 **mIoU** 性能。
- 当使用所有数据时，表现最佳，但训练设置较复杂。
- **仅使用自动生成的掩码**，性能仅比使用所有数据低 **0.5 mIoU**，差异很小。
- **结论**：为了简化训练设置，**默认使用自动生成的掩码进行训练**。

---
#### **(2) 数据量（Data Volume）**

- **数据量设置**：从完整的 **SA-1B 数据集（11M 图像）** 中进行子采样：
    - **1M 图像**（约占总数据集的10%）
    - **0.1M 图像**（约占总数据集的1%）
- **观察指标**：mIoU 变化。

**结果（Fig.13 中间）：**

- **0.1M 图像**：mIoU 显著下降，表明数据量不足以支持模型的泛化能力。
- **1M 图像**：性能与使用完整数据集时非常接近。
- **结论**：在大约 **1M 图像（约10%的数据量，约1亿个掩码）** 的情况下，性能已经接近最大化，对于资源有限的场景，这是一个**实用的训练规模**。

---
#### **(3) 模型架构（Model Architecture - ViT Encoders）**

- **对比的 ViT 模型**：
    - **ViT-B（Base）**
    - **ViT-L（Large）**
    - **ViT-H（Huge）**
- **观察指标**：不同 ViT 编码器对 SAM 性能的影响。

**结果（Fig.13 右侧）：**

- 从 **ViT-B → ViT-L**：性能显著提升。
- 从 **ViT-L → ViT-H**：性能提升幅度很小，几乎没有显著收益。
- **结论**：在当前配置下，**进一步增加编码器规模（ViT-H 以上）并不会带来显著性能提升**。

---
### **3. 关键发现（Key Findings）**

1. **数据源**
    
    - 使用**自动生成的掩码**进行训练几乎可以达到与全数据源训练相同的性能。
    - **默认设置**：为了简化训练流程，**SAM 默认仅使用自动生成的掩码进行训练**。
2. **数据量**
    
    - **1M 图像（约10%数据量）** 已经能够达到接近完整数据集的性能水平。
    - 在资源受限的情况下，这一规模是一个**合理且高效的训练配置**。
3. **模型架构**
    
    - **ViT-B → ViT-L**：性能显著提升。
    - **ViT-L → ViT-H**：性能提升有限，说明模型的性能已经接近规模瓶颈。
    - **结论**：在现有任务中，进一步扩大 ViT 模型的规模带来的性能提升微乎其微。

---

### **4. 总结（Summary）**

- **数据源**：默认使用**自动生成的掩码**，简化训练，性能损失极小。
- **数据量**：1M 图像（约10%的 SA-1B 数据集）已是一个**高效、实用的训练规模**。
- **模型架构**：**ViT-L** 是一个高效的选择，**ViT-H** 提升有限，进一步扩大编码器规模并不划算。

---
### **5. 实践建议（Practical Recommendations）**

- 在资源受限的场景中，使用 **1M 图像 + 自动生成掩码 + ViT-L 编码器** 是一个**高效、实用**的训练配置。
- 在需要最大性能的场景中，可以使用 **全量 SA-1B 数据 + ViT-H 编码器**，但增益相对较小。


# 网络结构细节：
图像编码器 通常，图像编码器可以是输出C×H×W图像嵌入块的任何网络。受可扩展性和强大的预训练的启发，作者使用MAE预训练的视觉transformer（ViT），具有最小的适应能力来处理高分辨率输入，特别是具有14×14窗口注意力和四个等距全局注意力块的ViT-H/16。图像编码器的输出是输入图像的16倍缩小的嵌入。由于运行时目标是实时处理每个提示，因此可以提供大量的图像编码器FLOP，因为每个图像只计算一次FLOP，而不是每个提示。
根据标准实践，使用1024×1024的输入分辨率，该分辨率是通过重新缩放图像并填充短边获得的。因此，图像嵌入是64×64。为了降低通道维度，使用1×1卷积来获得256个通道，然后使用3×3卷积来获得同样的256个通道。每个卷积后面都有一个层归一化。

提示编码器 稀疏提示被映射到256维矢量嵌入。一个点被表示为该点的位置的位置编码和指示该点是在前景中还是在背景中的两个学习嵌入之一的总和。方框由嵌入对表示：（1）其左上角的位置编码与表示“左上角”的学习嵌入相加；（2）相同的结构，但使用表示“右下角”的习得嵌入。最后，为了表示自由形式的文本，使用CLIP中的文本编码器（通常可以使用任何文本编码器）。密集提示（即掩码）与图像具有空间对应关系。以比输入图像低4倍的分辨率输入掩码，然后使用两个分别具有输出通道4和16的2×2，跨步-2卷积来缩小另外的4倍。最后的1×1卷积将通道维度映射到256。每一层通过GELU激活和层标准化进行分离。然后将掩码逐元素添加图像嵌入。如果没有掩码提示，则向每个图像嵌入位置添加表示“无掩码”的学习嵌入。

轻量级掩码解码器 该模块有效地将图像嵌入和一组提示嵌入映射到输出掩码。为了组合这些输入，从Transformer分割模型中获得灵感，并修改了标准Transformer解码器。在应用解码器之前，首先在提示嵌入集中插入一个学习的输出令牌嵌入，该嵌入将在解码器的输出中使用。为了简单起见，我们将这些嵌入（不包括图像嵌入）统称为“令牌”

![[mask decoder design.png]]

我们的解码器设计如上图所示。每个解码器层执行4个步骤：（1）对令牌的自注意力（2）从令牌（作为查询）到图像嵌入的交叉注意力（3）逐点MLP更新每个令牌，以及（4）从图像嵌入（作为查询的）到令牌的交叉关注。最后一步使用提示信息更新图像嵌入。在交叉关注期间，图像嵌入被视为642个256维向量的集合。每个自/交叉注意力和MLP在训练时都有残差连接、层归一化和dropout。下一解码器层从上一层获取更新的令牌和更新的图像嵌入。使用两层解码器。
为了确保解码器能够访问关键的几何信息，每当位置编码参与注意力层时，都会将其添加到图像嵌入中。此外，当整个原始提示标记（包括它们的位置编码）参与注意力层时，它们都会被重新添加到更新的标记中。这允许强烈依赖提示标记的几何位置和类型。
在运行解码器之后，用两个转置卷积将更新的图像嵌入上采样4×（现在它相对于输入图像缩小了4倍）。然后，令牌再次参与图像嵌入，并且将更新的输出令牌嵌入传递给小的3层MLP，该MLP输出与放大图像嵌入的通道维度匹配的向量。最后，预测了在放大图像嵌入和MLP的输出之间具有空间逐点乘积的掩模。转换器使用256的嵌入尺寸。转换器MLP块具有2048的大尺寸，但MLP仅应用于相对较少（很少大于20）的提示令牌。然而，在具有64×64图像嵌入的交叉关注层中，为了提高计算效率，将查询、键和值的通道维度降低了2×至128。所有注意力层使用8个头。用于升级输出图像嵌入的转置卷积是2×2，步长2，输出通道尺寸为64和32，并且具有GELU激活。它们通过层规范化来分隔。
